/* Machine mode switch for RISC-V 'V' Extension for GNU compiler.
   Copyright (C) 2022-2025 Free Software Foundation, Inc.
   Contributed by Ju-Zhe Zhong (juzhe.zhong@rivai.ai), RiVAI Technologies Ltd.

This file is part of GCC.

GCC is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 3, or (at your option)
any later version.

GCC is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with GCC; see the file COPYING3.  If not see
<http://www.gnu.org/licenses/>.  */

/* This file is enable or disable the RVV modes according '-march'.  */

/* According to rvv-intrinsic and RISC-V 'V' Extension ISA document:
   https://github.com/riscv-non-isa/rvv-intrinsic-doc/blob/master/rvv-intrinsic-rfc.md.
   https://github.com/riscv/riscv-v-spec/blob/master/v-spec.adoc.

   Data Types
   Encode SEW and LMUL into data types.
   We enforce the constraint LMUL ≥ SEW/ELEN in the implementation.
   There are the following data types for MIN_VLEN > 32.

Note: N/A means the corresponding vector type is disabled.

Encode SEW and LMUL into data types.
   We enforce the constraint LMUL ≥ SEW/ELEN in the implementation.
   There are the following data types for ELEN = 64.

   |Modes|LMUL=1 |LMUL=2 |LMUL=4 |LMUL=8 |LMUL=1/2|LMUL=1/4|LMUL=1/8|
   |DI   |RVVM1DI|RVVM2DI|RVVM4DI|RVVM8DI|N/A     |N/A     |N/A     |
   |SI   |RVVM1SI|RVVM2SI|RVVM4SI|RVVM8SI|RVVMF2SI|N/A     |N/A     |
   |HI   |RVVM1HI|RVVM2HI|RVVM4HI|RVVM8HI|RVVMF2HI|RVVMF4HI|N/A     |
   |QI   |RVVM1QI|RVVM2QI|RVVM4QI|RVVM8QI|RVVMF2QI|RVVMF4QI|RVVMF8QI|
   |DF   |RVVM1DF|RVVM2DF|RVVM4DF|RVVM8DF|N/A     |N/A     |N/A     |
   |SF   |RVVM1SF|RVVM2SF|RVVM4SF|RVVM8SF|RVVMF2SF|N/A     |N/A     |
   |HF   |RVVM1HF|RVVM2HF|RVVM4HF|RVVM8HF|RVVMF2HF|RVVMF4HF|N/A     |
   |BF   |RVVM1BF|RVVM2BF|RVVM4BF|RVVM8BF|RVVMF2BF|RVVMF4BF|N/A     |

There are the following data types for ELEN = 32.

  |Modes|LMUL=1 |LMUL=2 |LMUL=4 |LMUL=8 |LMUL=1/2|LMUL=1/4|LMUL=1/8|
  |SI   |RVVM1SI|RVVM2SI|RVVM4SI|RVVM8SI|N/A     |N/A     |N/A     |
  |HI   |RVVM1HI|RVVM2HI|RVVM4HI|RVVM8HI|RVVMF2HI|N/A     |N/A     |
  |QI   |RVVM1QI|RVVM2QI|RVVM4QI|RVVM8QI|RVVMF2QI|RVVMF4QI|N/A     |
  |SF   |RVVM1SF|RVVM2SF|RVVM4SF|RVVM8SF|N/A     |N/A     |N/A     |
  |HF   |RVVM1HF|RVVM2HF|RVVM4HF|RVVM8HF|RVVMF2HF|N/A     |N/A     |
  |BF   |RVVM1BF|RVVM2BF|RVVM4BF|RVVM8BF|RVVMF2BF|N/A     |N/A     |

Encode the ratio of SEW/LMUL into the mask types.
   There are the following mask types.

  n = SEW/LMUL

  |Modes| n = 1 | n = 2 | n = 4 | n = 8 | n = 16 | n = 32 | n = 64 |
  |BI   |RVVM1BI|RVVMF2BI|RVVMF4BI|RVVMF8BI|RVVMF16BI|RVVMF32BI|RVVMF64BI|  */

/* Return 'REQUIREMENT' for machine_mode 'MODE'.
   For example: 'MODE' = RVVMF64BImode needs TARGET_VECTOR_ELEN_64.  */
#ifndef ENTRY
#define ENTRY(MODE, REQUIREMENT, VLMUL, RATIO)
#endif

/* Disable modes if TARGET_MIN_VLEN == 32.  */
ENTRY (RVVMF64BI, TARGET_VECTOR_ELEN_64, TARGET_XTHEADVECTOR ? LMUL_1 :LMUL_F8, 64)
ENTRY (RVVMF32BI, true, TARGET_XTHEADVECTOR ? LMUL_1 :LMUL_F4, 32)
ENTRY (RVVMF16BI, true, TARGET_XTHEADVECTOR ? LMUL_1 : LMUL_F2 , 16)
ENTRY (RVVMF8BI, true, LMUL_1, 8)
ENTRY (RVVMF4BI, true, LMUL_2, 4)
ENTRY (RVVMF2BI, true, LMUL_4, 2)
ENTRY (RVVM1BI, true, LMUL_8, 1)

/* Disable modes if TARGET_MIN_VLEN == 32.  */
ENTRY (RVVM8QI, true, LMUL_8, 1)
ENTRY (RVVM4QI, true, LMUL_4, 2)
ENTRY (RVVM2QI, true, LMUL_2, 4)
ENTRY (RVVM1QI, true, LMUL_1, 8)
ENTRY (RVVMF2QI, !TARGET_XTHEADVECTOR, LMUL_F2, 16)
ENTRY (RVVMF4QI, !TARGET_XTHEADVECTOR, LMUL_F4, 32)
ENTRY (RVVMF8QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, LMUL_F8, 64)

/* Disable modes if TARGET_MIN_VLEN == 32.  */
ENTRY (RVVM8HI, true, LMUL_8, 2)
ENTRY (RVVM4HI, true, LMUL_4, 4)
ENTRY (RVVM2HI, true, LMUL_2, 8)
ENTRY (RVVM1HI, true, LMUL_1, 16)
ENTRY (RVVMF2HI, !TARGET_XTHEADVECTOR, LMUL_F2, 32)
ENTRY (RVVMF4HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, LMUL_F4, 64)

/* Disable modes if TARGET_MIN_VLEN == 32 or !TARGET_VECTOR_ELEN_BF_16.  */
ENTRY (RVVM8BF, TARGET_VECTOR_ELEN_BF_16, LMUL_8, 2)
ENTRY (RVVM4BF, TARGET_VECTOR_ELEN_BF_16, LMUL_4, 4)
ENTRY (RVVM2BF, TARGET_VECTOR_ELEN_BF_16, LMUL_2, 8)
ENTRY (RVVM1BF, TARGET_VECTOR_ELEN_BF_16, LMUL_1, 16)
ENTRY (RVVMF2BF, TARGET_VECTOR_ELEN_BF_16, LMUL_F2, 32)
ENTRY (RVVMF4BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, LMUL_F4, 64)

/* Disable modes if TARGET_MIN_VLEN == 32 or !TARGET_VECTOR_ELEN_FP_16.  */
ENTRY (RVVM8HF, TARGET_VECTOR_ELEN_FP_16, LMUL_8, 2)
ENTRY (RVVM4HF, TARGET_VECTOR_ELEN_FP_16, LMUL_4, 4)
ENTRY (RVVM2HF, TARGET_VECTOR_ELEN_FP_16, LMUL_2, 8)
ENTRY (RVVM1HF, TARGET_VECTOR_ELEN_FP_16, LMUL_1, 16)
ENTRY (RVVMF2HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, LMUL_F2, 32)
ENTRY (RVVMF4HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, LMUL_F4, 64)

/* Disable modes if TARGET_MIN_VLEN == 32.  */
ENTRY (RVVM8SI, true, LMUL_8, 4)
ENTRY (RVVM4SI, true, LMUL_4, 8)
ENTRY (RVVM2SI, true, LMUL_2, 16)
ENTRY (RVVM1SI, true, LMUL_1, 32)
ENTRY (RVVMF2SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, LMUL_F2, 64)

/* Disable modes if TARGET_MIN_VLEN == 32 or !TARGET_VECTOR_ELEN_FP_32.  */
ENTRY (RVVM8SF, TARGET_VECTOR_ELEN_FP_32, LMUL_8, 4)
ENTRY (RVVM4SF, TARGET_VECTOR_ELEN_FP_32, LMUL_4, 8)
ENTRY (RVVM2SF, TARGET_VECTOR_ELEN_FP_32, LMUL_2, 16)
ENTRY (RVVM1SF, TARGET_VECTOR_ELEN_FP_32, LMUL_1, 32)
ENTRY (RVVMF2SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, LMUL_F2, 64)

/* Disable modes if !TARGET_VECTOR_ELEN_64.  */
ENTRY (RVVM8DI, TARGET_VECTOR_ELEN_64, LMUL_8, 8)
ENTRY (RVVM4DI, TARGET_VECTOR_ELEN_64, LMUL_4, 16)
ENTRY (RVVM2DI, TARGET_VECTOR_ELEN_64, LMUL_2, 32)
ENTRY (RVVM1DI, TARGET_VECTOR_ELEN_64, LMUL_1, 64)

/* Disable modes if !TARGET_VECTOR_ELEN_FP_64.  */
ENTRY (RVVM8DF, TARGET_VECTOR_ELEN_FP_64, LMUL_8, 8)
ENTRY (RVVM4DF, TARGET_VECTOR_ELEN_FP_64, LMUL_4, 16)
ENTRY (RVVM2DF, TARGET_VECTOR_ELEN_FP_64, LMUL_2, 32)
ENTRY (RVVM1DF, TARGET_VECTOR_ELEN_FP_64, LMUL_1, 64)

/* Tuple modes for segment loads/stores according to NF.

   Tuple modes format: RVV<LMUL>x<NF><BASEMODE>

   When LMUL is MF8/MF4/MF2/M1, NF can be 2 ~ 8.
   When LMUL is M2, NF can be 2 ~ 4.
   When LMUL is M4, NF can be 4.  */

#ifndef TUPLE_ENTRY
#define TUPLE_ENTRY(MODE, REQUIREMENT, SUBPART_MODE, NF, VLMUL, RATIO)
#endif

TUPLE_ENTRY (RVVM1x8QI, true, RVVM1QI, 8, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x8QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 8, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x8QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 8, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x8QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 8, LMUL_F8, 64)
TUPLE_ENTRY (RVVM1x7QI, true, RVVM1QI, 7, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x7QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 7, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x7QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 7, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x7QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 7, LMUL_F8, 64)
TUPLE_ENTRY (RVVM1x6QI, true, RVVM1QI, 6, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x6QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 6, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x6QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 6, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x6QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 6, LMUL_F8, 64)
TUPLE_ENTRY (RVVM1x5QI, true, RVVM1QI, 5, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x5QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 5, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x5QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 5, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x5QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 5, LMUL_F8, 64)
TUPLE_ENTRY (RVVM2x4QI, true, RVVM2QI, 4, LMUL_2, 4)
TUPLE_ENTRY (RVVM1x4QI, true, RVVM1QI, 4, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x4QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 4, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x4QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 4, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x4QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 4, LMUL_F8, 64)
TUPLE_ENTRY (RVVM2x3QI, true, RVVM2QI, 3, LMUL_2, 4)
TUPLE_ENTRY (RVVM1x3QI, true, RVVM1QI, 3, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x3QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 3, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x3QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 3, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x3QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 3, LMUL_F8, 64)
TUPLE_ENTRY (RVVM4x2QI, true, RVVM4QI, 2, LMUL_4, 2)
TUPLE_ENTRY (RVVM2x2QI, true, RVVM2QI, 2, LMUL_2, 4)
TUPLE_ENTRY (RVVM1x2QI, true, RVVM1QI, 2, LMUL_1, 8)
TUPLE_ENTRY (RVVMF2x2QI, !TARGET_XTHEADVECTOR, RVVMF2QI, 2, LMUL_F2, 16)
TUPLE_ENTRY (RVVMF4x2QI, !TARGET_XTHEADVECTOR, RVVMF4QI, 2, LMUL_F4, 32)
TUPLE_ENTRY (RVVMF8x2QI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF8QI, 2, LMUL_F8, 64)

TUPLE_ENTRY (RVVM1x8HI, true, RVVM1HI, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x8HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 8, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x8HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 8, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x7HI, true, RVVM1HI, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x7HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 7, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x7HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 7, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x6HI, true, RVVM1HI, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x6HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 6, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x6HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 6, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x5HI, true, RVVM1HI, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x5HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 5, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x5HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 5, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x4HI, true, RVVM2HI, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4HI, true, RVVM1HI, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x4HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 4, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x4HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 4, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x3HI, true, RVVM2HI, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3HI, true, RVVM1HI, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x3HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 3, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x3HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 3, LMUL_F4, 64)
TUPLE_ENTRY (RVVM4x2HI, true, RVVM4HI, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2HI, true, RVVM2HI, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2HI, true, RVVM1HI, 2, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x2HI, !TARGET_XTHEADVECTOR, RVVMF2HI, 2, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x2HI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HI, 2, LMUL_F4, 64)

TUPLE_ENTRY (RVVM1x8BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x8BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 8, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x8BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 8, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x7BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x7BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 7, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x7BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 7, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x6BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x6BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 6, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x6BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 6, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x5BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x5BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 5, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x5BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 5, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x4BF, TARGET_VECTOR_ELEN_BF_16, RVVM2BF, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x4BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 4, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x4BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 4, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x3BF, TARGET_VECTOR_ELEN_BF_16, RVVM2BF, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x3BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 3, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x3BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 3, LMUL_F4, 64)
TUPLE_ENTRY (RVVM4x2BF, TARGET_VECTOR_ELEN_BF_16, RVVM4BF, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2BF, TARGET_VECTOR_ELEN_BF_16, RVVM2BF, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2BF, TARGET_VECTOR_ELEN_BF_16, RVVM1BF, 2, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x2BF, TARGET_VECTOR_ELEN_BF_16, RVVMF2BF, 2, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x2BF, TARGET_VECTOR_ELEN_BF_16 && TARGET_MIN_VLEN > 32, RVVMF4BF, 2, LMUL_F4, 64)

TUPLE_ENTRY (RVVM1x8HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x8HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 8, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x8HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 8, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x7HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x7HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 7, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x7HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 7, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x6HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x6HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 6, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x6HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 6, LMUL_F4, 64)
TUPLE_ENTRY (RVVM1x5HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x5HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 5, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x5HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 5, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x4HF, TARGET_VECTOR_ELEN_FP_16, RVVM2HF, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x4HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 4, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x4HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 4, LMUL_F4, 64)
TUPLE_ENTRY (RVVM2x3HF, TARGET_VECTOR_ELEN_FP_16, RVVM2HF, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x3HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 3, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x3HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 3, LMUL_F4, 64)
TUPLE_ENTRY (RVVM4x2HF, TARGET_VECTOR_ELEN_FP_16, RVVM4HF, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2HF, TARGET_VECTOR_ELEN_FP_16, RVVM2HF, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2HF, TARGET_VECTOR_ELEN_FP_16, RVVM1HF, 2, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x2HF, TARGET_VECTOR_ELEN_FP_16 && !TARGET_XTHEADVECTOR, RVVMF2HF, 2, LMUL_F2, 32)
TUPLE_ENTRY (RVVMF4x2HF, TARGET_VECTOR_ELEN_FP_16 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF4HF, 2, LMUL_F4, 64)

TUPLE_ENTRY (RVVM1x8SI, true, RVVM1SI, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x8SI, (TARGET_VECTOR_ELEN_64) && !TARGET_XTHEADVECTOR, RVVMF2SI, 8, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x7SI, true, RVVM1SI, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x7SI, (TARGET_VECTOR_ELEN_64) && !TARGET_XTHEADVECTOR, RVVMF2SI, 7, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x6SI, true, RVVM1SI, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x6SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SI, 6, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x5SI, true, RVVM1SI, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x5SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SI, 5, LMUL_F2, 32)
TUPLE_ENTRY (RVVM2x4SI, true, RVVM2SI, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4SI, true, RVVM1SI, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x4SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SI, 4, LMUL_F2, 32)
TUPLE_ENTRY (RVVM2x3SI, true, RVVM2SI, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3SI, true, RVVM1SI, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x3SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SI, 3, LMUL_F2, 32)
TUPLE_ENTRY (RVVM4x2SI, true, RVVM4SI, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2SI, true, RVVM2SI, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2SI, true, RVVM1SI, 2, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x2SI, TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SI, 2, LMUL_F2, 32)

TUPLE_ENTRY (RVVM1x8SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x8SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 8, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x7SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x7SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 7, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x6SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x6SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 6, LMUL_F2, 32)
TUPLE_ENTRY (RVVM1x5SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x5SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 5, LMUL_F2, 32)
TUPLE_ENTRY (RVVM2x4SF, TARGET_VECTOR_ELEN_FP_32, RVVM2SF, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x4SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 4, LMUL_F2, 32)
TUPLE_ENTRY (RVVM2x3SF, TARGET_VECTOR_ELEN_FP_32, RVVM2SF, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x3SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 3, LMUL_F2, 32)
TUPLE_ENTRY (RVVM4x2SF, TARGET_VECTOR_ELEN_FP_32, RVVM4SF, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2SF, TARGET_VECTOR_ELEN_FP_32, RVVM2SF, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2SF, TARGET_VECTOR_ELEN_FP_32, RVVM1SF, 2, LMUL_1, 16)
TUPLE_ENTRY (RVVMF2x2SF, TARGET_VECTOR_ELEN_FP_32 && TARGET_VECTOR_ELEN_64 && !TARGET_XTHEADVECTOR, RVVMF2SF, 2, LMUL_F2, 32)

TUPLE_ENTRY (RVVM1x8DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x7DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x6DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x5DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVM2x4DI, TARGET_VECTOR_ELEN_64, RVVM2DI, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVM2x3DI, TARGET_VECTOR_ELEN_64, RVVM2DI, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVM4x2DI, TARGET_VECTOR_ELEN_64, RVVM4DI, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2DI, TARGET_VECTOR_ELEN_64, RVVM2DI, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2DI, TARGET_VECTOR_ELEN_64, RVVM1DI, 2, LMUL_1, 16)

TUPLE_ENTRY (RVVM1x8DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 8, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x7DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 7, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x6DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 6, LMUL_1, 16)
TUPLE_ENTRY (RVVM1x5DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 5, LMUL_1, 16)
TUPLE_ENTRY (RVVM2x4DF, TARGET_VECTOR_ELEN_FP_64, RVVM2DF, 4, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x4DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 4, LMUL_1, 16)
TUPLE_ENTRY (RVVM2x3DF, TARGET_VECTOR_ELEN_FP_64, RVVM2DF, 3, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x3DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 3, LMUL_1, 16)
TUPLE_ENTRY (RVVM4x2DF, TARGET_VECTOR_ELEN_FP_64, RVVM4DF, 2, LMUL_4, 4)
TUPLE_ENTRY (RVVM2x2DF, TARGET_VECTOR_ELEN_FP_64, RVVM2DF, 2, LMUL_2, 8)
TUPLE_ENTRY (RVVM1x2DF, TARGET_VECTOR_ELEN_FP_64, RVVM1DF, 2, LMUL_1, 16)

#ifndef VLS_ENTRY
#define VLS_ENTRY(MODE, REQUIREMENT)
#endif

/* This following VLS modes should satisfy the constraint:
   GET_MODE_BITSIZE (MODE) <= TARGET_MIN_VLEN * 8.  */
VLS_ENTRY (V1BI, riscv_vector::vls_mode_valid_p (V1BImode))
VLS_ENTRY (V2BI, riscv_vector::vls_mode_valid_p (V2BImode))
VLS_ENTRY (V4BI, riscv_vector::vls_mode_valid_p (V4BImode))
VLS_ENTRY (V8BI, riscv_vector::vls_mode_valid_p (V8BImode))
VLS_ENTRY (V16BI, riscv_vector::vls_mode_valid_p (V16BImode))
VLS_ENTRY (V32BI, riscv_vector::vls_mode_valid_p (V32BImode))
VLS_ENTRY (V64BI, riscv_vector::vls_mode_valid_p (V64BImode) && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V128BI, riscv_vector::vls_mode_valid_p (V128BImode) && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V256BI, riscv_vector::vls_mode_valid_p (V256BImode) && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V512BI, riscv_vector::vls_mode_valid_p (V512BImode) && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V1024BI, riscv_vector::vls_mode_valid_p (V1024BImode) && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V2048BI, riscv_vector::vls_mode_valid_p (V2048BImode) && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V4096BI, riscv_vector::vls_mode_valid_p (V4096BImode) && TARGET_MIN_VLEN >= 4096)

VLS_ENTRY (V1QI, riscv_vector::vls_mode_valid_p (V1QImode))
VLS_ENTRY (V2QI, riscv_vector::vls_mode_valid_p (V2QImode))
VLS_ENTRY (V4QI, riscv_vector::vls_mode_valid_p (V4QImode))
VLS_ENTRY (V8QI, riscv_vector::vls_mode_valid_p (V8QImode))
VLS_ENTRY (V16QI, riscv_vector::vls_mode_valid_p (V16QImode))
VLS_ENTRY (V32QI, riscv_vector::vls_mode_valid_p (V32QImode))
VLS_ENTRY (V64QI, riscv_vector::vls_mode_valid_p (V64QImode) && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V128QI, riscv_vector::vls_mode_valid_p (V128QImode) && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V256QI, riscv_vector::vls_mode_valid_p (V256QImode) && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V512QI, riscv_vector::vls_mode_valid_p (V512QImode) && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V1024QI, riscv_vector::vls_mode_valid_p (V1024QImode) && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V2048QI, riscv_vector::vls_mode_valid_p (V2048QImode) && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V4096QI, riscv_vector::vls_mode_valid_p (V4096QImode) && TARGET_MIN_VLEN >= 4096)
VLS_ENTRY (V1HI, riscv_vector::vls_mode_valid_p (V1HImode))
VLS_ENTRY (V2HI, riscv_vector::vls_mode_valid_p (V2HImode))
VLS_ENTRY (V4HI, riscv_vector::vls_mode_valid_p (V4HImode))
VLS_ENTRY (V8HI, riscv_vector::vls_mode_valid_p (V8HImode))
VLS_ENTRY (V16HI, riscv_vector::vls_mode_valid_p (V16HImode))
VLS_ENTRY (V32HI, riscv_vector::vls_mode_valid_p (V32HImode) && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V64HI, riscv_vector::vls_mode_valid_p (V64HImode) && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V128HI, riscv_vector::vls_mode_valid_p (V128HImode) && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V256HI, riscv_vector::vls_mode_valid_p (V256HImode) && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V512HI, riscv_vector::vls_mode_valid_p (V512HImode) && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V1024HI, riscv_vector::vls_mode_valid_p (V1024HImode) && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V2048HI, riscv_vector::vls_mode_valid_p (V2048HImode) && TARGET_MIN_VLEN >= 4096)
VLS_ENTRY (V1SI, riscv_vector::vls_mode_valid_p (V1SImode))
VLS_ENTRY (V2SI, riscv_vector::vls_mode_valid_p (V2SImode))
VLS_ENTRY (V4SI, riscv_vector::vls_mode_valid_p (V4SImode))
VLS_ENTRY (V8SI, riscv_vector::vls_mode_valid_p (V8SImode))
VLS_ENTRY (V16SI, riscv_vector::vls_mode_valid_p (V16SImode) && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V32SI, riscv_vector::vls_mode_valid_p (V32SImode) && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V64SI, riscv_vector::vls_mode_valid_p (V64SImode) && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V128SI, riscv_vector::vls_mode_valid_p (V128SImode) && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V256SI, riscv_vector::vls_mode_valid_p (V256SImode) && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V512SI, riscv_vector::vls_mode_valid_p (V512SImode) && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V1024SI, riscv_vector::vls_mode_valid_p (V1024SImode) && TARGET_MIN_VLEN >= 4096)
VLS_ENTRY (V1DI, riscv_vector::vls_mode_valid_p (V1DImode) && TARGET_VECTOR_ELEN_64)
VLS_ENTRY (V2DI, riscv_vector::vls_mode_valid_p (V2DImode) && TARGET_VECTOR_ELEN_64)
VLS_ENTRY (V4DI, riscv_vector::vls_mode_valid_p (V4DImode) && TARGET_VECTOR_ELEN_64)
VLS_ENTRY (V8DI, riscv_vector::vls_mode_valid_p (V8DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V16DI, riscv_vector::vls_mode_valid_p (V16DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V32DI, riscv_vector::vls_mode_valid_p (V32DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V64DI, riscv_vector::vls_mode_valid_p (V64DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V128DI, riscv_vector::vls_mode_valid_p (V128DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V256DI, riscv_vector::vls_mode_valid_p (V256DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V512DI, riscv_vector::vls_mode_valid_p (V512DImode) && TARGET_VECTOR_ELEN_64 && TARGET_MIN_VLEN >= 4096)

VLS_ENTRY (V1HF, riscv_vector::vls_mode_valid_p (V1HFmode) && TARGET_VECTOR_ELEN_FP_16)
VLS_ENTRY (V2HF, riscv_vector::vls_mode_valid_p (V2HFmode) && TARGET_VECTOR_ELEN_FP_16)
VLS_ENTRY (V4HF, riscv_vector::vls_mode_valid_p (V4HFmode) && TARGET_VECTOR_ELEN_FP_16)
VLS_ENTRY (V8HF, riscv_vector::vls_mode_valid_p (V8HFmode) && TARGET_VECTOR_ELEN_FP_16)
VLS_ENTRY (V16HF, riscv_vector::vls_mode_valid_p (V16HFmode) && TARGET_VECTOR_ELEN_FP_16)
VLS_ENTRY (V32HF, riscv_vector::vls_mode_valid_p (V32HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V64HF, riscv_vector::vls_mode_valid_p (V64HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V128HF, riscv_vector::vls_mode_valid_p (V128HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V256HF, riscv_vector::vls_mode_valid_p (V256HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V512HF, riscv_vector::vls_mode_valid_p (V512HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V1024HF, riscv_vector::vls_mode_valid_p (V1024HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V2048HF, riscv_vector::vls_mode_valid_p (V2048HFmode) && TARGET_VECTOR_ELEN_FP_16 && TARGET_MIN_VLEN >= 4096)
VLS_ENTRY (V1SF, riscv_vector::vls_mode_valid_p (V1SFmode) && TARGET_VECTOR_ELEN_FP_32)
VLS_ENTRY (V2SF, riscv_vector::vls_mode_valid_p (V2SFmode) && TARGET_VECTOR_ELEN_FP_32)
VLS_ENTRY (V4SF, riscv_vector::vls_mode_valid_p (V4SFmode) && TARGET_VECTOR_ELEN_FP_32)
VLS_ENTRY (V8SF, riscv_vector::vls_mode_valid_p (V8SFmode) && TARGET_VECTOR_ELEN_FP_32)
VLS_ENTRY (V16SF, riscv_vector::vls_mode_valid_p (V16SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V32SF, riscv_vector::vls_mode_valid_p (V32SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V64SF, riscv_vector::vls_mode_valid_p (V64SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V128SF, riscv_vector::vls_mode_valid_p (V128SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V256SF, riscv_vector::vls_mode_valid_p (V256SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V512SF, riscv_vector::vls_mode_valid_p (V512SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V1024SF, riscv_vector::vls_mode_valid_p (V1024SFmode) && TARGET_VECTOR_ELEN_FP_32 && TARGET_MIN_VLEN >= 4096)
VLS_ENTRY (V1DF, riscv_vector::vls_mode_valid_p (V1DFmode) && TARGET_VECTOR_ELEN_FP_64)
VLS_ENTRY (V2DF, riscv_vector::vls_mode_valid_p (V2DFmode) && TARGET_VECTOR_ELEN_FP_64)
VLS_ENTRY (V4DF, riscv_vector::vls_mode_valid_p (V4DFmode) && TARGET_VECTOR_ELEN_FP_64)
VLS_ENTRY (V8DF, riscv_vector::vls_mode_valid_p (V8DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 64)
VLS_ENTRY (V16DF, riscv_vector::vls_mode_valid_p (V16DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 128)
VLS_ENTRY (V32DF, riscv_vector::vls_mode_valid_p (V32DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 256)
VLS_ENTRY (V64DF, riscv_vector::vls_mode_valid_p (V64DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 512)
VLS_ENTRY (V128DF, riscv_vector::vls_mode_valid_p (V128DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 1024)
VLS_ENTRY (V256DF, riscv_vector::vls_mode_valid_p (V256DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 2048)
VLS_ENTRY (V512DF, riscv_vector::vls_mode_valid_p (V512DFmode) && TARGET_VECTOR_ELEN_FP_64 && TARGET_MIN_VLEN >= 4096)

#undef VLS_ENTRY
#undef TUPLE_ENTRY
#undef ENTRY
